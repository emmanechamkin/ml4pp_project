{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "TBD\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Public perception of the police is incredibly important to police effectiveness and legitimacy but extremely difficult to measure. Public perception offers insight into how well a police department is functioning and may suggest adherence to tenets of procedural justice. Yet, compared to traditional performance metrics, metrics to evaluate public opinion are poorly defined and documented. \n",
    "\n",
    "There is very little research currently available that measures real-world public sentiment of the police in US cities. As a result, this project serves to provide a proof-of-concept first and foremost that publicly-available data can be easily acquired and use in order to study common issues in policing. \n",
    "\n",
    "## Clearance rates\n",
    "\n",
    "Homicide clearance rates, or what share of murders a police department \"solves\", are a key performance metric for police departments. Chicago has one of the lowest homicide clearance rates in the country, and only about 1 in 6 murders lead to arrest. Moreover, Chicago's clearance rate has steadily declined over the past ten years, from about 40% in 2000 down to under 20% in 2017. As a comparison, several police departments have markedly higher clearance rates. Over the past decade, Los Angeles has solved 51% of murders and New York has solved 61% of murders. \n",
    "\n",
    "There are several potential reasons for the low clearance rate in Chicago, some of which suggest that non-traditional metrics of policing like procedural justice or public opinion may be related to traditional metrics. Police officers tend to cite the historically fraught relationship between the people and police, believing that someone who already views the police negatively because police seem inept may be less likely to cooperate with an investigation; more bluntly, many police officers lament a \"no snitch\" policy among victimized communities in Chicago. Evidence is conflicted: the National Crime Victimization Survey reports that these communities are no less likely to report crimes to the police, but a Cato Institute survey shows a race and education gap for crime reporting. There are also other viable explanations for Chicago's abysmal clearance rate, most notably that Chicago's police force has limited manpower per murder. Chicago has more murders than New York and Los Angeles combined, yet the police department (12,000 officers) is dwarved by New York's (36,000) and Los Angeles' (10,000).\n",
    "\n",
    "## Public opinion\n",
    "\n",
    "Public opinion can also help measure procedural justice, or how police officers enforce laws.  Although procedural justice is difficult to measure directly, past research has evaluated procedural justice through the lens of public opinion survey data.   \n",
    "\n",
    "Procedural justice is necessary for effective policing. A civilian who considers the law enforcement process fair and just is likely to consider any related consequences fair and just, too. Conversely, when civilians perceive lack of procedural justice, they are more likely to file complaints and view their police force as delegitimate. For example, one study of New York Police Department Stop, Question, and Frisk stops showed that civilians who believed their stop to be fair were less likely to file a complaint than those who believed their stop was unjust. Finally, a lack of procedural justice in just a few encouters can severely curtail public opinion of the police. Negative interactions with the police shape citizen perception up to fourteen times more strongly than positive ones. \n",
    "\n",
    "Public perception of the police offers an additional metric to assess police performance. While hard metrics like clearance rates are easy to measure, assessing how the public feels towards the police is far more complex. Indeed, most work that tries to assess public sentiment uses survey-based or experimental research. More recent work has considered sentiments of tweets to assess public opinion of the police. As a caveat, public perception of the police is complicated and interacts with policing in myriad ways.\n",
    "\n",
    "## Research goals\n",
    "\n",
    "The goal of this work is to assess the extent to which twitter data can be reliably used to evaluate public opinion of the police department. There is no dataset on police-related tweets that I could draw on, for example; very little by way of classifying tweets as police-related or not exists at present. This work then serves to primarily explore whether tweets can be reliably be categorized as \"police relevant, positive\", \"police relevant, negative\", \"police relevant, neutral\", or \"not police relevant\" with respect to public sentiment. \n",
    "\n",
    "Why might such work be important? There are thousands of tweets about policing each and every day in the United States. Understanding which tweets reflect public sentiment (rather than are unrelated but use similar acronyms) provide a foundation for further research. More importantly, understanding what precise sentiment tweets express on a larger scale can better enable researchers to measure public sentiment of the police. \n",
    "\n",
    "More concretely, this is simply the beginning. Once we can reliably predict how a tweet relates to policing, we can begin to assess public perception of the police. More specifically, I'm interested in assessing the extent to which public sentiment reflects traditional metrics of police effectiveness, where effectiveness here is roughly equivalent to clearance.\n",
    "\n",
    "# Past Work - UPDATE from Amitabh comments\n",
    "\n",
    "## Using twitter data to measure public sentiment towards the police \n",
    "\n",
    "Although there has been limited work using data science techniques to study criminal justice, the Urban Institute used sentiment analysis for police-related tweets to measure how perception of the police changed due to the murder of Freddie Gray, using the following methods:\n",
    "\n",
    "- Obtaining the data: researchers used a set of relevant tweets from 2014 and 2015 acquired through twitter.\n",
    "- Processing the data: researchers removed mentions, hashtags, links, punctuation, and stop words from all tweets. They also used CoreNLP to tag tweets (e.g., to identify whether \"cop\" was a noun or a verb in each tweet).\n",
    "- Learning models: researchers classified over 4,000 tweets manually to identify whether the tweet was positive, negative, neutral, or not applicable to their research for use in training and validation sets. They then used several types of models to predict the sentiment of new tweets and selected a gradient-boosted regression classifier as their model based on its accuracy (63%). \n",
    "- Conclusions: researchers then used their newly labeled set of all tweets to assess the shift in public sentiment over time. \n",
    "\n",
    "## Using twitter data to connect public opinion with tweet sentiment\n",
    "\n",
    "As a more general example, researchers at Carnegie Mellon University determined that public opinion surveys correlate to twitter sentiment on several key issues. They used twitter data specifically with two endgoals: to identify relevant tweets and to estimate sentiment (positive and negative) about a given topic. \n",
    "\n",
    "In their work, researchers obtained tweets from 2008 and 2009 using the twitter API.  They then used key words (like \"obama\" to measure presidential approval) to ensure that their tweets were relevant. Tweets were classified as positive, negative, or both depending on whether there was a positive, a negative, or both types of words in it. Finally, to get an accurate measure of sentiment, they computed a moving average aggregate of sentiment ratios, where sentiment ratio was defined as the ratio between the number of positive and negative relevant tweets. The moving averages allowed them to smooth otherwise volatile data. They then investigated correlations between the sentiment they uncovered and traditional public opinion surveys. \n",
    "\n",
    "# Methods \n",
    "\n",
    "Unlike my wiser classmates, I decided to create my own dataset using twitter data. Ultimately, data cleaning took an enormous effort -- and I could continue to clean data forever and incrementally improve data quality with each run. Below, I walk through the decisions I made when creating my dataset, as well as some tradeoffs each decision posed. \n",
    "\n",
    "## Extracting tweets using the Twitter API\n",
    "\n",
    "Tweets were extracted from the Twitter API using the following criteria:\n",
    "- Tweets contained at least one of the following search terms: \"Chicago Police\", \"CPD\", \"chicago police department\", \"second city cop\", \"chicago cop\"\n",
    "- Tweets were extracted in two rounds: one in early April that was overly dominated by the Jurnee Smollet case, and one at the middle of May. \n",
    "\n",
    "Ultimately, tweets in the second set were used for modelling. Each dataset included over 15,000 tweets because the first dataset was weighted too heavily in terms of the Smollett case, an incident in which a famous actor allegedly falsely reported a hate crime. I did not want my dataset to be weighted heavily in terms of a single topic; rather, I wanted data to be more general. \n",
    "\n",
    "As an aside: In retrospect, all news cycles have certain dominant stories. Were I to execute this project again, I would have used a sample of tweets from many time periods (say, 150/week over a period of a year) in order to ensure that my model was not simply learning how certain news events are labeled. \n",
    "\n",
    "## Preprocessing and labeling tweets \n",
    "\n",
    "Preprocessing and labeling large amounts of text data posed a non-trivial challenge. I attempted several approaches, as listed below, in order to efficiently and accurately label the data. I used the following criteria to label tweets:\n",
    "\n",
    "- _Not Relevant_: A tweet that my filters picked up, but was not actually about the police (e.g., it used a common acronym) was coded as not relevant\n",
    "- _Neutral_: Neutral tweets discussed facts or headlines. Any news headline or simple fact (e.g., \"Police Union Meeting Tomorrow at 6p!\") was classified as neutral\n",
    "- _Positive_: A tweet that expressed someone's positive view of the police was classified as positive\n",
    "- _Negative_: A tweet that expressed someone's negative view of the police was classified as negative\n",
    "\n",
    "To preprocess, I used the following pipeline:\n",
    "- I stemmed all words\n",
    "- I removed punctuation and irregular characters\n",
    "- I removed citations for retweets (the handles that followed) and twitter handles where possible\n",
    "- I removed hashtag symbols\n",
    "- I attempted to translate emoji into English meanings\n",
    "\n",
    "Note that depending on the way I handled data labeling, I was able to maintain different aspects of the tweets. I have flagged important compromises where I made them. Across the board, there were several data processing issues that I had to content with across methods:\n",
    "- Emojis: in order to import all data, I needed to use \"mac roman\" encoding. This eliminated a lot of emoji data. As a result, emojis, which I had originally intended to translate to their corresponding words, could not be included across all datasets\n",
    "- Tweet characters: For long tweets or tweets with retweet citations, not all of the data could be included in the text. Instead, these tweets became truncated. \n",
    "\n",
    "### Hand labeling \n",
    "First, I tried to hand label a subset of tweets using these rules. Not only was this wildly inefficient, I also learned that I myself have bias. I didn't want my own bias to impact how I coded a tweet (for example, I noticed that I would code \"bad\" news stories as negative if someone retweeted them, but not \"good\" news stories). I also have experience working with the police, and so read significant subtext in tweets where subtext might not have been present (for example, if a tweet mentioned Second City Cop, a popular sarcastic blog that is pro-police, I often interpreted the tweet as sarcastic). Consequently, I decided that I would not hand label all tweets independently. \n",
    "\n",
    "### CoreNLP\n",
    "The next thing I attempted was to use an out-of-the-box sentiment classifier to identify positive and negative tweets about the police. To do this, I configured an aws EC2 instance such that it was running Stanford's CoreNLP program on port 9000, and then submitted cleaned tweet text to the server for all unique tweets. Once I had my cleaned data, I examined the automated results. \n",
    "\n",
    "This approach posed several challenges. First, and perhaps most importantly, it did not address the key issue (bias) from hand labeling. The out-of-the-box senitment labeling was problematic because it didn't fully account for context. As a result, many of the tweets were mischaracterized. Second of all, this approach was even more time intensive than labeling, since I had to set up the EC2 server, \n",
    "\n",
    "As an example, the following tweet was classified as \"negative\". Because this tweet is a headline, it should have been classified as \"neutral\". This problem was pervasive throughout the dataset, and because tweets had already been preprocessed before getting sent to the CoreNLP server, extremely difficult to fix. What's more, fixing the issue would be akin to hand-labeling tweets, which is what I wanted to avoid in the first place. \n",
    "\n",
    "``` chicago il tribune local chicago police searching two men briefly abducted 12yearold girl south side```\n",
    "\n",
    "### MTurk HIT task\n",
    "The final method that I used to label - and the one I completed and eventually used - was to create a Human Intelligence Task on Amazon's MTurk to have workers label my text. \n",
    "\n",
    "MTurk is a platform that connects requesters to workers. A requester is someone who wants a certain task completed by humans (e.g., labeling text). A worker gets paid per correct task completion. In this case, I submitted somewhat clean (but not stemmed) unique tweets to MTurk (a bit over 1500). The screens that workers saw are shown below. \n",
    "\n",
    "![alt text](images/MTurk1)\n",
    "![alt text](images/MTurk2)\n",
    "\n",
    "## Classifying tweets as relevant or irrelevant \n",
    "\n",
    "## Feature engineering \n",
    "\n",
    "## Modeling \n",
    "\n",
    "# Results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selected sources\n",
    "\n",
    "## Papers\n",
    "\n",
    "Ekins, Emily. (2016). Policing in America: Understanding Public Attitudes Toward the Police. Results from a National Survey. SSRN Electronic Journal. 10.2139/ssrn.2919449. \n",
    "\n",
    "Fowler, AF Rengifo and K. 2016. \"Stop, Question, and Complain: Citizen Grievances Against the NYPD and the Opacity of Police Stops Across New York City Precincts, 2007-2013.\" Journal of Urban Health (93 Suppl 1): 32-41.\n",
    "\n",
    "O'Connor, Brendan & Balasubramanyan, Ramnath & R. Routledge, Bryan & A. Smith, Noah. (2010). From Tweets to Polls: Linking Text Sentiment to Public Opinion Time Series. International AAAI Conference on Weblogs and Social Media. 11. \n",
    "\n",
    "Skogan, Wesley G. 2006. \"Asymmetry in the Impact of Encounters With Police.\" Policing and Society.\n",
    "\n",
    "Tyler, Tom R. 2004. \"Enhancing Police Legitimacy.\" The Annals of the American Academy of Political and Social Science 593: 84-99.\n",
    "\n",
    "## News and articles (quick links)\n",
    "\n",
    "- https://www.washingtonpost.com/graphics/2018/investigations/unsolved-homicide-database/?utm_term=.8da8a801878a&city=indianapolis]\n",
    "- https://chicago.suntimes.com/news/murder-clearance-rate-in-chicago-hit-new-low-in-2017/\n",
    "- https://www.theatlantic.com/ideas/archive/2018/05/quis-custodiet-ipsos-custodes/560324/\n",
    "- https://datasmart.ash.harvard.edu/news/article/map-monday-unsolved-homicides"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
