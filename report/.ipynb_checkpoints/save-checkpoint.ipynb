{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "TBD\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Public perception of the police is incredibly important to police effectiveness and legitimacy but extremely difficult to measure. Public perception offers insight into how well a police department is functioning and may suggest adherence to tenets of procedural justice. Yet, compared to traditional performance metrics, metrics to evaluate public opinion are poorly defined and documented. \n",
    "\n",
    "There is very little research currently available that measures real-world public sentiment of the police in US cities. As a result, this project serves to provide a proof-of-concept first and foremost that publicly-available data can be easily acquired and use in order to study common issues in policing. \n",
    "\n",
    "## Clearance rates\n",
    "\n",
    "Homicide clearance rates, or what share of murders a police department \"solves\", are a key performance metric for police departments. Chicago has one of the lowest homicide clearance rates in the country, and only about 1 in 6 murders lead to arrest. Moreover, Chicago's clearance rate has steadily declined over the past ten years, from about 40% in 2000 down to under 20% in 2017. As a comparison, several police departments have markedly higher clearance rates. Over the past decade, Los Angeles has solved 51% of murders and New York has solved 61% of murders. \n",
    "\n",
    "There are several potential reasons for the low clearance rate in Chicago, some of which suggest that non-traditional metrics of policing like procedural justice or public opinion may be related to traditional metrics. Police officers tend to cite the historically fraught relationship between the people and police, believing that someone who already views the police negatively because police seem inept may be less likely to cooperate with an investigation; more bluntly, many police officers lament a \"no snitch\" policy among victimized communities in Chicago. Evidence is conflicted: the National Crime Victimization Survey reports that these communities are no less likely to report crimes to the police, but a Cato Institute survey shows a race and education gap for crime reporting. There are also other viable explanations for Chicago's abysmal clearance rate, most notably that Chicago's police force has limited manpower per murder. Chicago has more murders than New York and Los Angeles combined, yet the police department (12,000 officers) is dwarved by New York's (36,000) and Los Angeles' (10,000).\n",
    "\n",
    "## Public opinion\n",
    "\n",
    "Public opinion can also help measure procedural justice, or how police officers enforce laws.  Although procedural justice is difficult to measure directly, past research has evaluated procedural justice through the lens of public opinion survey data.   \n",
    "\n",
    "Procedural justice is necessary for effective policing. A civilian who considers the law enforcement process fair and just is likely to consider any related consequences fair and just, too. Conversely, when civilians perceive lack of procedural justice, they are more likely to file complaints and view their police force as delegitimate. For example, one study of New York Police Department Stop, Question, and Frisk stops showed that civilians who believed their stop to be fair were less likely to file a complaint than those who believed their stop was unjust. Finally, a lack of procedural justice in just a few encouters can severely curtail public opinion of the police. Negative interactions with the police shape citizen perception up to fourteen times more strongly than positive ones. \n",
    "\n",
    "Public perception of the police offers an additional metric to assess police performance. While hard metrics like clearance rates are easy to measure, assessing how the public feels towards the police is far more complex. Indeed, most work that tries to assess public sentiment uses survey-based or experimental research. More recent work has considered sentiments of tweets to assess public opinion of the police. As a caveat, public perception of the police is complicated and interacts with policing in myriad ways.\n",
    "\n",
    "## Research goals\n",
    "\n",
    "The goal of this work is to assess the extent to which twitter data can be reliably used to evaluate public opinion of the police department. There is no dataset on police-related tweets that I could draw on, for example; very little by way of classifying tweets as police-related or not exists at present. This work then serves to primarily explore whether tweets can be reliably be categorized as \"police relevant, positive\", \"police relevant, negative\", \"police relevant, neutral\", or \"not police relevant\" with respect to public sentiment. \n",
    "\n",
    "Why might such work be important? There are thousands of tweets about policing each and every day in the United States. Understanding which tweets reflect public sentiment (rather than are unrelated but use similar acronyms) provide a foundation for further research. More importantly, understanding what precise sentiment tweets express on a larger scale can better enable researchers to measure public sentiment of the police. \n",
    "\n",
    "More concretely, this is simply the beginning. Once we can reliably predict how a tweet relates to policing, we can begin to assess public perception of the police. More specifically, I'm interested in assessing the extent to which public sentiment reflects traditional metrics of police effectiveness, where effectiveness here is roughly equivalent to clearance.\n",
    "\n",
    "# Past Work \n",
    "\n",
    "## Using twitter data to measure public sentiment towards the police \n",
    "\n",
    "Although there has been limited work using data science techniques to study criminal justice, the Urban Institute used sentiment analysis for police-related tweets to measure how perception of the police changed due to the murder of Freddie Gray, using the following methods:\n",
    "\n",
    "- Obtaining the data: researchers used a set of relevant tweets from 2014 and 2015 acquired through twitter.\n",
    "- Processing the data: researchers removed mentions, hashtags, links, punctuation, and stop words from all tweets. They also used CoreNLP to tag tweets (e.g., to identify whether \"cop\" was a noun or a verb in each tweet).\n",
    "- Learning models: researchers classified over 4,000 tweets manually to identify whether the tweet was positive, negative, neutral, or not applicable to their research for use in training and validation sets. They then used several types of models to predict the sentiment of new tweets and selected a gradient-boosted regression classifier as their model based on its accuracy (63%). \n",
    "- Conclusions: researchers then used their newly labeled set of all tweets to assess the shift in public sentiment over time. \n",
    "\n",
    "## Using twitter data to connect public opinion with tweet sentiment\n",
    "\n",
    "As a more general example, researchers at Carnegie Mellon University determined that public opinion surveys correlate to twitter sentiment on several key issues. They used twitter data specifically with two endgoals: to identify relevant tweets and to estimate sentiment (positive and negative) about a given topic. The specific methods used in this paper are less relevant to this work than the fact that twitter data alone was used to measure public sentiment. \n",
    "\n",
    "# Methods \n",
    "\n",
    "Unlike my wiser classmates, I decided to create my own dataset using twitter data. Ultimately, data cleaning took an enormous effort -- and I could continue to clean data forever and incrementally improve data quality with each run. Below, I walk through the decisions I made when creating my dataset, as well as some tradeoffs each decision posed. \n",
    "\n",
    "## Extracting tweets using the Twitter API\n",
    "\n",
    "Tweets were extracted from the Twitter API using the following criteria:\n",
    "\n",
    "- Tweets contained at least one of the following search terms: \"Chicago Police\", \"CPD\", \"chicago police department\", \"second city cop\", \"chicago cop\"\n",
    "- Tweets were extracted in two rounds: one in early April that was overly dominated by the Jurnee Smollet case, and one at the middle of May. \n",
    "\n",
    "Ultimately, tweets in the second set were used for modelling. Each dataset included over 15,000 tweets because the first dataset was weighted too heavily in terms of the Smollett case, an incident in which a famous actor allegedly falsely reported a hate crime. I did not want my dataset to be weighted heavily in terms of a single topic; rather, I wanted data to be more general. \n",
    "\n",
    "As an aside: In retrospect, all news cycles have certain dominant stories. Were I to execute this project again, I would have used a sample of tweets from many time periods (say, 150/week over a period of a year) in order to ensure that my model was not simply learning how certain news events are labeled. \n",
    "\n",
    "## Preprocessing and labeling tweets \n",
    "\n",
    "Preprocessing and labeling large amounts of text data posed a non-trivial challenge. I attempted several approaches, as listed below, in order to efficiently and accurately label the data. I used the following criteria to label tweets:\n",
    "\n",
    "- _Not Relevant_: A tweet that my filters picked up, but was not actually about the police (e.g., it used a common acronym) was coded as not relevant\n",
    "- _Neutral_: Neutral tweets discussed facts or headlines. Any news headline or simple fact (e.g., \"Police Union Meeting Tomorrow at 6p!\") was classified as neutral\n",
    "- _Positive_: A tweet that expressed someone's positive view of the police was classified as positive\n",
    "- _Negative_: A tweet that expressed someone's negative view of the police was classified as negative\n",
    "\n",
    "To preprocess, I used the following pipeline:\n",
    "\n",
    "- I stemmed all words\n",
    "- I removed punctuation and irregular characters\n",
    "- I removed citations for retweets (the handles that followed) and twitter handles where possible\n",
    "- I removed hashtag symbols\n",
    "- I attempted to translate emoji into English meanings\n",
    "\n",
    "Note that depending on the way I handled data labeling, I was able to maintain different aspects of the tweets. I have flagged important compromises where I made them. Across the board, there were several data processing issues that I had to content with across methods:\n",
    "\n",
    "- Emojis: in order to import all data, I needed to use \"mac roman\" encoding. This eliminated a lot of emoji data. As a result, emojis, which I had originally intended to translate to their corresponding words, could not be included across all datasets\n",
    "- Tweet characters: For long tweets or tweets with retweet citations, not all of the data could be included in the text. Instead, these tweets became truncated. \n",
    "\n",
    "### Hand labeling \n",
    "First, I tried to hand label a subset of tweets using these rules. Not only was this wildly inefficient, I also learned that I myself have bias. I didn't want my own bias to impact how I coded a tweet (for example, I noticed that I would code \"bad\" news stories as negative if someone retweeted them, but not \"good\" news stories). I also have experience working with the police, and so read significant subtext in tweets where subtext might not have been present (for example, if a tweet mentioned Second City Cop, a popular sarcastic blog that is pro-police, I often interpreted the tweet as sarcastic). Consequently, I decided that I would not hand label all tweets independently. \n",
    "\n",
    "### CoreNLP\n",
    "The next thing I attempted was to use an out-of-the-box sentiment classifier to identify positive and negative tweets about the police. To do this, I configured an aws EC2 instance such that it was running Stanford's CoreNLP program on port 9000, and then submitted cleaned tweet text to the server for all unique tweets. Once I had my cleaned data, I examined the automated results. \n",
    "\n",
    "This approach posed several challenges. First, and perhaps most importantly, it did not address the key issue (bias) from hand labeling. The out-of-the-box senitment labeling was problematic because it didn't fully account for context. As a result, many of the tweets were mischaracterized. Second of all, this approach was even more time intensive than labeling, since I had to set up the EC2 server, \n",
    "\n",
    "As an example, the following tweet was classified as \"negative\". Because this tweet is a headline, it should have been classified as \"neutral\". This problem was pervasive throughout the dataset, and because tweets had already been preprocessed before getting sent to the CoreNLP server, extremely difficult to fix. What's more, fixing the issue would be akin to hand-labeling tweets, which is what I wanted to avoid in the first place. \n",
    "\n",
    "_tweet:_ ``` chicago il tribune local chicago police searching two men briefly abducted 12yearold girl south side```\n",
    "\n",
    "### MTurk HIT task\n",
    "The final method that I used to label - and the one I completed and eventually used - was to create a Human Intelligence Task on Amazon's MTurk to have workers label my text. \n",
    "\n",
    "MTurk is a platform that connects requesters to workers. A requester is someone who wants a certain task completed by humans (e.g., labeling text). A worker gets paid per correct task completion. In this case, I submitted somewhat clean (but not stemmed) unique tweets to MTurk (a bit over 1500). The screens that workers saw are shown below. \n",
    "\n",
    "__Decision screen__ <br>\n",
    "\n",
    "![This is the screen that all MTurk workers saw when classifying tweets](images/MTurk2.png)\n",
    "\n",
    "__Additional instructions__ <br>\n",
    "\n",
    "![These were the instructions that I provided for the task. Workers could refer to these instructions at any time](images/MTurk1.png)\n",
    "\n",
    "Once workers classified all of the tweets, I reviewed the work. I rejected over 800 tweets (about half of the original task load) due to blatant inaccuracies (e.g., facts as positively or negatively coded). In total, about 2400 tweets were coded by MTurk workers, of which I used about 1500 in model building. \n",
    "\n",
    "This was my first time using MTurk. Were I to do it again, I would change several things:\n",
    "\n",
    "- I would require worker qualifications to ensure that only good quality workers were coding my tweets. Reviewing the coding took an enormous amount of time. Not only did I have to review almost every single tweet, I also had to write explanations for every tweet that I rejected and correspond with workers about why their work was rejected. \n",
    "- I would have each tweet coded by 3 high-quality workers as a way to flag irregularly coded tweets. I had to review every single tweet for this project, but had I had the means to ensure multiple eyes on each tweet, I surely would have, and would only have checked tweets with disagreement. \n",
    "- I would have been clearer in my instructions by providing specific examples. I think many workers did not read my instructions carefully, and I believe my instructions were, in retrospect, vague. I would more carefully describe the task and also be sure to more clearly label different options for each tweet. \n",
    "\n",
    "## Final dataset\n",
    "\n",
    "Across all data, the tweets were classified as:\n",
    "\n",
    "| Label        | Count | Percent |\n",
    "|--------------|-------|---------|\n",
    "| Not relevant | 439   | 28.6%   |\n",
    "| Neutral      | 541   | 35.2%   |\n",
    "| Negative     | 287   | 18.7%   |\n",
    "| Positive     | 270   | 17.6%   |\n",
    "\n",
    "The dataset was split 50/50 into training and testing data. For the neural networks, the validation set was taken by random from the training set. For the more traditional models, there was no validation, as I didn't tune hyperparameters. In future work, once I have improved these models, I will also include validation. \n",
    "\n",
    "My priority, however, with this run of the model was to increase my training set as much as possible in order to enable me to have as robust as possible data. This ended up being the right choice -- my models became less precise as I decreased the training set size -- even if untraditional. \n",
    "\n",
    "In the training set, the tweets were classified as:\n",
    "\n",
    "| Label        | Count | Percent |\n",
    "|--------------|-------|---------|\n",
    "| Not relevant | 242   | 31.5%   |\n",
    "| Neutral      | 257   | 33.4%   |\n",
    "| Negative     | 135   | 17.4%   |\n",
    "| Positive     | 134   | 17.6%   |\n",
    "| Total        | 768   | 100%    |\n",
    "\n",
    "And in the testing set, the tweets were classified as:\n",
    "\n",
    "| Label        | Count | Percent |\n",
    "|--------------|-------|---------|\n",
    "| Not relevant | 197   | 31.5%   |\n",
    "| Neutral      | 284   | 33.4%   |\n",
    "| Negative     | 152   | 17.4%   |\n",
    "| Positive     | 135   | 17.6%   |\n",
    "| Total        | 768   | 100%    |\n",
    "\n",
    "These are important baseline characteristics for the models to be compared to. \n",
    "\n",
    "The data that I found departed significantly from what Urban Institute found. In the Urban Institute work, about 2% of tweets were classified as positive; I found a much higher rate. I do not know why this would be the case; to investigate, I would like to look at national-level data in the future in order to assess whether I can reproduce distributions similar to the Urban Institute's. For now, I can hope that this data was skewed positive due to:\n",
    "\n",
    "- Community involvement increasing\n",
    "- Positive police responses to high-publicity crimes\n",
    "- Lori Lightfoot's election, which may herald police reform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "I tried several ways to gain insight on and model the data. In this section of the report, I will first walk through some general observations that I have about the data, then briefly summarize the models that I ran, and then describe future work that I could do to improve these predictions. \n",
    "\n",
    "I ran a lot of models (on the order of 50, if you include changing hyperparameters) and will report on the overall trends that I found. Additional detail that is less salient can be found in my code. \n",
    "\n",
    "## General observations about the data\n",
    "\n",
    "\n",
    "### Word counts\n",
    "I first wanted to get a good sense of what in particular was in my data. First, I looked at words that occurred most commonly across tweets:\n",
    "\n",
    "| Word         | Number of tweets included in |\n",
    "|--------------|------------------------------|\n",
    "| chicago      | 417                          |\n",
    "| police       | 410                          |\n",
    "| chicagopolic | 119                          |\n",
    "| pregnant     | 96                           |\n",
    "| babi         | 87                           |\n",
    "| woman        | 86                           |\n",
    "| say          | 69                           |\n",
    "| miss         | 69                           |\n",
    "| found        | 65                           |\n",
    "| womb         | 58                           |\n",
    "| synagogu     | 54                           |\n",
    "| cut          | 48                           |\n",
    "| cwbchicago   | 40                           |\n",
    "| dead         | 40                           |\n",
    "| arrest       | 38                           |\n",
    "| amp          | 38                           |\n",
    "| report       | 37                           |\n",
    "| peopl        | 35                           |\n",
    "| home         | 35                           |\n",
    "| charge       | 32                           |\n",
    "\n",
    "Some of these words are general, like `chicago` and `chicagopolic` and `arrest`. Others relate to specific news stories, like `pregnant` and `synagogu` (there was a murder of a pregnant woman and a molotov cocktail thrown at a synagogue the week that I pulled tweets). This hints at a larger problem with the data: any week-long time period is not sufficiently general to build a general model on. This was good information for me to learn. \n",
    "\n",
    "This effect is even more pronounced when bigrams are considered. The most popular bigrams here were: `babi cut`, `chicago police`, `cut womb`, `cwbcchicago chicagopolic`, `found dead`, `miss pregnant`, `polic say`, and `pregnant woman`. \n",
    "\n",
    "Another important note is that these tweets varied greatly. Even the most common words appeared in only about half of tweets. This led me to cluster my data. \n",
    "\n",
    "### Similarity and clustering \n",
    "\n",
    "In order to further understand how similar and different tweets are, I next used cosine similarity on a TF-IDF matrix from my data in order to assess how similar my tweets were to each other. The short answer is that most tweets are different from most other tweets. \n",
    "\n",
    "To investigate this further, I used my similarity matrix to build clusters. This was highly influenced by a tutorial that I saw online, and this tutorial is cited in my code. \n",
    "\n",
    "More than half of my clusters included fewer than 10 tweets, which was not ideal. The most popular clusters, also, were less than ideal. They showed mixed sentiment by cluster, and the most dominant cluster predominantly had irrelevant tweets. The results from the four most popular clusters are shown below, where the number of tweet per label by cluster is shown. \n",
    "\n",
    "![Clusters with more than 20 members](images/clusters.png)\n",
    "\n",
    "These results are otherwise difficult to interpret, as many of the processed tweets are difficult to comprehend. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selected sources\n",
    "\n",
    "## Papers\n",
    "\n",
    "Ekins, Emily. (2016). Policing in America: Understanding Public Attitudes Toward the Police. Results from a National Survey. SSRN Electronic Journal. 10.2139/ssrn.2919449. \n",
    "\n",
    "Fowler, AF Rengifo and K. 2016. \"Stop, Question, and Complain: Citizen Grievances Against the NYPD and the Opacity of Police Stops Across New York City Precincts, 2007-2013.\" Journal of Urban Health (93 Suppl 1): 32-41.\n",
    "\n",
    "O'Connor, Brendan & Balasubramanyan, Ramnath & R. Routledge, Bryan & A. Smith, Noah. (2010). From Tweets to Polls: Linking Text Sentiment to Public Opinion Time Series. International AAAI Conference on Weblogs and Social Media. 11. \n",
    "\n",
    "Skogan, Wesley G. 2006. \"Asymmetry in the Impact of Encounters With Police.\" Policing and Society.\n",
    "\n",
    "Tyler, Tom R. 2004. \"Enhancing Police Legitimacy.\" The Annals of the American Academy of Political and Social Science 593: 84-99.\n",
    "\n",
    "## News and articles (quick links)\n",
    "\n",
    "- https://www.washingtonpost.com/graphics/2018/investigations/unsolved-homicide-database/?utm_term=.8da8a801878a&city=indianapolis]\n",
    "- https://chicago.suntimes.com/news/murder-clearance-rate-in-chicago-hit-new-low-in-2017/\n",
    "- https://www.theatlantic.com/ideas/archive/2018/05/quis-custodiet-ipsos-custodes/560324/\n",
    "- https://datasmart.ash.harvard.edu/news/article/map-monday-unsolved-homicides"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
