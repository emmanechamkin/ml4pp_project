{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/emmanechamkin/miniconda3/envs/advml/bin/python\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "\n",
    "import random\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## help setting my kernel\n",
    "https://janakiev.com/til/jupyter-virtual-envs/\n",
    "```(advml) Emmas-MacBook-Pro-2:Project emmanechamkin$ python -m ipykernel install --user --name=advml\n",
    "Installed kernelspec advml in /Users/emmanechamkin/Library/Jupyter/kernels/advml```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work so far\n",
    "Data here has already been preprocessed (in tweet_extraction) and labeled. I changed tactic and then had MTurk do it for me. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import emoji\n",
    "from emoji.unicode_codes import UNICODE_EMOJI\n",
    "import csv\n",
    "import random\n",
    "\n",
    "# use keras tokenizer instead\n",
    "import keras\n",
    "import keras.preprocessing.text as kpt\n",
    "from keras.preprocessing.text import Tokenizer \n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = pd.read_csv(OUTPUT_FILE, encoding=\"mac-roman\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HITId', 'HITTypeId', 'Title', 'Description', 'Keywords', 'Reward',\n",
       "       'CreationTime', 'MaxAssignments', 'RequesterAnnotation',\n",
       "       'AssignmentDurationInSeconds', 'AutoApprovalDelayInSeconds',\n",
       "       'Expiration', 'NumberOfSimilarHITs', 'LifetimeInSeconds',\n",
       "       'AssignmentId', 'WorkerId', 'AssignmentStatus', 'AcceptTime',\n",
       "       'SubmitTime', 'AutoApprovalTime', 'ApprovalTime', 'RejectionTime',\n",
       "       'RequesterFeedback', 'WorkTimeInSeconds', 'LifetimeApprovalRate',\n",
       "       'Last30DaysApprovalRate', 'Last7DaysApprovalRate', 'Input.text',\n",
       "       'Answer.sentiment.label', 'Approve', 'Reject'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = temp[temp.AssignmentStatus != \"Rejected\"]\n",
    "temp = temp[['HITId', 'Answer.sentiment.label', 'Input.text']]\n",
    "\n",
    "# take the most common result\n",
    "final = temp.groupby(['HITId','Input.text']).agg(lambda x:x.value_counts().index[0])\n",
    "\n",
    "# print to a file\n",
    "final.to_csv(CLEANED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## temporary cleaning\n",
    "def parse(word):\n",
    "    word = word.strip().lower().translate(TABLE)\n",
    "    if word in UNICODE_EMOJI:\n",
    "        word = re.sub(\"_\", \" \", UNICODE_EMOJI[word][1:-1]).lower()\n",
    "    if (word.startswith(\"rt\")\n",
    "            or word.startswith(\"@\")\n",
    "            or \"http\" in word\n",
    "            or word in STOP_WORDS):\n",
    "        word = \"\"\n",
    "    if word.startswith(\"#\"):\n",
    "        word = word[1:]\n",
    "    return word\n",
    "\n",
    "\n",
    "STOP_WORDS = set(stopwords.words('english'))\n",
    "TABLE = str.maketrans({key: None for key in string.punctuation})\n",
    "STEMMER = PorterStemmer()\n",
    "\n",
    "with open(CLEANED, \"r\") as csv_file, \\\n",
    "    open(FINAL, \"w\", newline=\"\") as out_file:\n",
    "        reader = csv.reader(csv_file, delimiter=\",\")\n",
    "        writer = csv.writer(out_file, delimiter=\",\")\n",
    "        for row in reader:\n",
    "            tweet = row[1]\n",
    "            tweet_keep = \"\"\n",
    "            for word in tweet.split():\n",
    "                # get rid of all non-ascii characters\n",
    "                word = re.sub(r'[^\\x00-\\x7f]', r'', word)\n",
    "                # clean\n",
    "                word = parse(word)\n",
    "                # stem\n",
    "                word_stem = STEMMER.stem(word)\n",
    "                # save\n",
    "                if word_stem:\n",
    "                    tweet_keep += \" \" + word_stem\n",
    "            if tweet_keep:\n",
    "                try:\n",
    "                    writer.writerow([row[0], tweet_keep, row[2]])\n",
    "                except Exception as e: #add error here\n",
    "                    print(\"Error\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>inputtext</th>\n",
       "      <th>Answer.sentiment.label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HITId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>301KG0KX9CMDD7S33IQR6Z489RA2HI</th>\n",
       "      <td>artsprison williamay convers wthe author tort...</td>\n",
       "      <td>About the police - Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301KG0KX9CMDD7S33IQR6Z489RAH2X</th>\n",
       "      <td>imalibrarian chicagosmayor dad lifelong chica...</td>\n",
       "      <td>About the police - Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301KG0KX9CMDD7S33IQR6Z489RB2HJ</th>\n",
       "      <td>chicago polic swat offic respond report arm i...</td>\n",
       "      <td>About the police - Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302OLP89DZ8B12CRU5551QQQ8NXACO</th>\n",
       "      <td>almost forgot foia turn one year old today hb...</td>\n",
       "      <td>Not about the police</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302OLP89DZ8B12CRU5551QQQ8NXCAQ</th>\n",
       "      <td>three peopl charg miss pregnant woman found d...</td>\n",
       "      <td>Not about the police</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                        inputtext  \\\n",
       "HITId                                                                               \n",
       "301KG0KX9CMDD7S33IQR6Z489RA2HI   artsprison williamay convers wthe author tort...   \n",
       "301KG0KX9CMDD7S33IQR6Z489RAH2X   imalibrarian chicagosmayor dad lifelong chica...   \n",
       "301KG0KX9CMDD7S33IQR6Z489RB2HJ   chicago polic swat offic respond report arm i...   \n",
       "302OLP89DZ8B12CRU5551QQQ8NXACO   almost forgot foia turn one year old today hb...   \n",
       "302OLP89DZ8B12CRU5551QQQ8NXCAQ   three peopl charg miss pregnant woman found d...   \n",
       "\n",
       "                                     Answer.sentiment.label  \n",
       "HITId                                                        \n",
       "301KG0KX9CMDD7S33IQR6Z489RA2HI  About the police - Positive  \n",
       "301KG0KX9CMDD7S33IQR6Z489RAH2X   About the police - Neutral  \n",
       "301KG0KX9CMDD7S33IQR6Z489RB2HJ   About the police - Neutral  \n",
       "302OLP89DZ8B12CRU5551QQQ8NXACO         Not about the police  \n",
       "302OLP89DZ8B12CRU5551QQQ8NXCAQ         Not about the police  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in the data \n",
    "tweet_data = pd.read_csv(FINAL, index_col=0)\n",
    "tweet_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "About the police - Neutral     541\n",
       "Not about the police           442\n",
       "About the police - Negative    290\n",
       "About the police - Positive    264\n",
       "Name: Answer.sentiment.label, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_data.iloc[:, 1].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### train / validate / test split \n",
    "tweet_id = set(tweet_data.index)\n",
    "total = len(tweet_id)\n",
    "train_frac = 0.5\n",
    "validate_frac = 0.5\n",
    "# I ended up using validate as test for normal, and built-in \n",
    "# validation for NNs, so I increased this so my models would work :)\n",
    "\n",
    "train_size = int(total*train_frac)\n",
    "validate_size = int(total*validate_frac)\n",
    "\n",
    "train_id = random.sample(tweet_id, train_size)\n",
    "tweet_id = tweet_id.difference(train_id)\n",
    "\n",
    "validate_id = random.sample(tweet_id, validate_size)\n",
    "tweet_id = tweet_id.difference(validate_id)\n",
    "\n",
    "test_id = tweet_id\n",
    "\n",
    "train_df = tweet_data[tweet_data.index.isin(train_id)]\n",
    "validate_df = tweet_data[tweet_data.index.isin(validate_id)]\n",
    "test_df = tweet_data[tweet_data.index.isin(test_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "About the police - Neutral     273\n",
       "Not about the police           223\n",
       "About the police - Positive    138\n",
       "About the police - Negative    134\n",
       "Name: Answer.sentiment.label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for data hygeine -- to check that this \n",
    "# is the same over time\n",
    "tweet_sentiment = train_df.iloc[:, 1]\n",
    "tweet_sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that the TRAIN set is pretty big -- I know it's bigger than we would traditionally want. Because I ended up using a lot of CV and validation inside of the NN's, I had to split data less for the NNs and wanted to ensure that my models had maximum data. \n",
    "\n",
    "Also, my models were unfortunately so bad that this didn't end up being a huge problem..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_csv(TRAIN, index=False)\n",
    "validate_df.to_csv(VALIDATE, index=False)\n",
    "test_df.to_csv(TEST, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "advml",
   "language": "python",
   "name": "advml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
